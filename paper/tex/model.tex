\section{Our Covert Communication System Model}
\label{s:model}
In this section, we begin with an overview of our covert communication model and will further discuss the details of our scheme in the subsequent sections.
\subsection{Overview}
The main idea of our work is to establish a covert channel on top of a normal communication between a sender (UserTX) and a receiver (UserRX) who are using an autoencoder-based wireless system to communicate. We consider both an AWGN and a Rayleigh fading channels between our communication parties. The objective of the covert sender (Alice) is to secretly communicate with the covert receiver (Bob) by embedding her messages in form of perturbations that have similar statistical properties as of the channel's noise. This is achieved by help of an observer (Willie), which acts as an observer of the channel, trying to rigorously classify transmitted signals as covert and non-covert after they pass through the channel.\\
\subsection{System Architecture}
As mentioned above, there are three main actors in our covert system which we call them by their placeholder names: Alice, Bob, and Willie from now on. All three are collaborating in our covert scheme and are represented by DNNs. Alice is using a generative model that embeds a confidential message \(m\) into a covert noise vector \(\hat{z}\). This covert signal is then transmitted over the channel after being added to a normal signal \(x\). Similar to every covert communication scheme, there is an observer or warden in the system that will be alerted when seeing any deviation in the statistical properties of the channel. To this end, we are implicitly incorporating a statistical undetectability constraint on the produced covert signals by having a discriminator network employed by Willie. The presence of this discriminator network helps us to ensure that these added covert noise signals are distributed as the real channel noise, making them undetectable. Mathematically, in the AWGN channel model, it means \(\hat{z} \sim \mathcal{CN}(0, \sigma_{chl}^2)\) where \(\sigma_{chl}^2\) will be the variance of the channel's noise.\\
For a given binary secret message \(m\), Alice first one-hot encodes the message and then uses its generator model to produce a covert noise signal \(\hat{z}\). This covert signal is then added to a vector of a normal signal, which is carrying messages between UserTX and UserRX. Therefore, the covert signals before being transmitted over the channel can be denoted as:
\begin{equation}
	\hat{x} = x + \hat{z}
\end{equation}
The signal is then transmitted over the channel. We mentioned that we assume the channel between sender and receiver of the system to be AWGN or Rayleigh. Therefore, there will be two different channel outputs for these two different channel models that can be represented as a mapping function \(C(\cdot)\):
\subsubsection{AWGN Channel Output}
For the AWGN channel model, the signal received at the receiver carries within itself the channel noise effect \(z \sim \mathcal{N}(0, \sigma_{chl}^2)\). Thus, the channel function \(C(\cdot)\) and final covert signal \(\hat{y}\) can be represented as:
\begin{equation}
	C(\hat{x}) \Rightarrow \hat{y} = \hat{x} + z
\end{equation}
\subsubsection{Rayleigh Fading Channel Output}
For the Rayleigh fading channel model, we consider a flat block fading channel where each codeword is assumed to be faded independently. Let \(h\) be the fading coefficient when transmitting the codeword \(\hat{x}\), then the channel function \(C(.)\) and and the final covert signal \(\hat{y}\) is given by:
\begin{equation}
	C(\hat{x}) \Rightarrow \hat{y} = h \cdot \hat{x} + z
\end{equation}
On the receiver side, Bob receives a transmitted signal \(\hat{y}\) after the channel noise is added into it. He uses its decoder network to reconstruct the covert message \(\hat{m}\). Meanwhile, the UserRX is using the same signal to extract the normal message \(\hat{s}\), which is the reconstructed message of \(s\) sent from UserTX. The statistical properties of the signals on the channel are captured by Willie who observes the channel continuously. His objective is to classify sequences of normal \(y\) and covert signals \(\hat{y}\) passed through the channel and provide useful feedback to Alice. This feedback helps Alice to modify the produced covert signals such that they are indistinguishable from the normal signals of the communication. In other words, it ensures that both normal and covert signals have the same statistical properties.
\subsection{General Formulation}
The very first objective of our covert model is to have a working covert channel. To this end, Bob has to have a plausible accuracy in decoding covert messages that Alice sends through the covert signals \(\hat{y}\). As mentioned in previous section, Alice employs a generative model instead of an encoder model suggested by \cite{mohammed2021adversarial}. Using an encoder model to produce covert signal perturbations will map each covert message \(m\) to a single covert noise vector \(\hat{z}\). Inevitably, these deterministic covert perturbations can be eliminated with ease by a careful observer or defender using one of the techniques proposed in the work of Bahramali et al. \cite{bahramali2021robust}, who already studied a similar problem statement. Thus, we use an stochastic generative model for Alice so that each covert message maps to a set of different covert noise signals. Let \(A(\cdot)\) be the underlying function of Alice's generative model that takes a random trigger \(t \sim \mathcal{N}(0, 1)\) and a covert message \(m\)) and produce a covert signal \(\hat{z}\)) (we denote the corresponding covert signal as \(\hat{z}_{m, t} = A(m, t)\)). Let also  \(B(\cdot)\) be the underlying function of the decoder network that Bob makes use of to reconstruct the covert message \(\hat{m}\). Then the reliability of communication between Alice and Bob is achieved using the below loss function:
\begin{equation}
	\begin{array}{l} \label{alice_bob_loss}
	\mathcal{L}_{Alice-Bob} = \mathbb{E}_{m}[CE(\hat{m}, m)] \\ \Rightarrow \mathbb{E}_{m}[CE(B(C(A(m, t) + x)), m)]
	\end{array}
\end{equation}
where \(CE(\cdot)\) is the cross entropy between the reconstructed covert message \(\hat{m}\) and the actual covert message \(m\). This equation can be used to optimize both for Alice's and Bob's networks by freezing one or the other network's parameters iteratively. While (\ref{alice_bob_loss}) ensures the communication accuracy, we also need to mind that the generated perturbations should have no detrimental impact on the normal communication between UserTX and UserRX. This constraint is applied by optimizing for the below loss function during training Alice's generator network:
\begin{equation}
	\begin{array}{l}
	\mathcal{L}_{Alice \mid UserRX} = \mathbb{E}_{m}[CE(\hat{s}, s)] \\ \Rightarrow \mathbb{E}_{m}[CE(D(C(A(m, t) + E(s))), s)]
	\end{array}
\end{equation}
where \(D(\cdot)\) is UserRX's decoder network function, and \(E(\cdot)\) is the underlying function of the UserTX's encoder network. Note that both UserTX's encoder and UserRX's decoder networks are frozen during this training and only Alice's parameters are updated.\\
In our model, the observer entity or Willie, act as the discriminator in GAN models \cite{goodfellow2014generative}. The so-called real and fake data in GANs' discriminator training here is mapped to non-covert and covert signals and we define the loss function as:
\begin{equation}
		\begin{array}{l}
	\mathcal{L}_{Willie} = \mathbb{E}_{m}[BCE(\hat{y}, y)] \\ \Rightarrow \mathbb{E}_{m}[BCE(C(A(m,t) + x), C(x))]
	\end{array}
\end{equation}
where \(BCE(\cdot)\) is the binary cross entropy between the covert signal \(\hat{y}\) and the normal signal \(y\). This adversarial training against Alice network ensures that Willie will be adequately trained to tell covert and non-covert signals apart. On the other hand, we mentioned that the covert signals that Alice produces should not deviate from the statistical properties of the normal signals on the channel, otherwise the observer of the channel can detect and mitigate the covert communication. To achieve this undetectability property, we pose a new constraint on Alice's optimization function for maximizing Willie's uncertainty about the covert predictions. Having a regularizer as such helps Alice and Bob to form their covert communication in way that is indistinguishable from the actual channel's noise, yet understandable by both. Altogether, Alice's loss function can be expressed as:
\begin{equation}
	\begin{array}{l} \label{alice_bob_loss}
	\mathcal{L}_{Alice} = \mathcal{L}_{Alice-Bob} + \mathcal{L}_{Alice \mid UserRX} - \mathcal{L}_{Willie}
\end{array}
\end{equation}

\subsection{Neural Network Architecture}
Before discussing the architecture of our neural network models, we need to state that the communication between UserTX and UserRX is taking place using an autoencoder communication model, which is utilized to encode and decode the normal messages between these two entities. Since the focus of this work is not to introduce an autoencoder wireless network, we briefly explains how this model works. Also, an overview of the network's architecture of implemented autoencoder model can be found in table (?). Detailed explanation of such a network or the training procedure can be found in the original paper \cite{o2017introduction}. The other three entities of our scheme, which are our covert actors, also employ neural networks to function. A detailed network architecture of each is likewise represented in table (?).\\
Similar to the original paper, our autoencoder model accepts a binary message \(s\) of size \(k\) bits. Afterwards, the encoder part first one-hot encodes the message and then maps it to a vector of signals of size \(2 \times n\), where \(n\) is the number of channel uses. This transmitted signal is then given to a mapping function that applies the channel effects into it. On the receiver side, there will be two different structures given what the channel model is. In case of an AWGN channel model, the signal is simply passed through the decoder network and the intended message gets extracted. For the Rayleigh fading channel model, however, there will be a parameter estimation model that takes in the signal before getting passed through the decoder network in order to estimate the channel's fading coefficients from it. Followed by that, the extracted signal along with the estimated parameters are passed to a transformation function that is supposed to revert the channel effects. In our case, we are using a simple division transformation function that divides the received signal by the estimated channel fading coefficients. More complex transformation functions can be used and are described in \cite{o2017introduction}. Eventually, the transformed signal is fed to the decoder network and the original normal message is reconstructed. Similar to the encoder network, Alice takes a covert message \(m\) and transforms it to an one-hot encoding representation. Next, given a random trigger \(t\), it uses its generator model to produce a covert noise signal \(\hat{z}\) and adds it to the normal signal \(x\) that is being transmitted. Bob receives this covert signal after passing through the channel and feeds it through its decoder network regardless of what the channel is and extracts the secret message. Willie also receives the same covert signal \(\hat{y}\) and the non-covert signal \(y\) and outputs a confidence probability \(P\) on how probable it is that the signal is normal. 
