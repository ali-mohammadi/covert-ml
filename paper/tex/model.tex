\section{System Model}
\label{s:model}
Our system consists of two types of users: normal users who communicate with each other via autoencoder wireless systems and covert users who aim to establish hidden communication in the system without arousing suspicion from the observer. The communication between normal users can be either single-user, i.e., between a transmitter and a receiver, or multi-user, i.e., between multiple transmitters and a base station receiver.

We will start by describing our system model in the single-user case, which simplifies the system by eliminating the complexity of interference between entities. This scenario applies to systems that already handle user interference at higher levels or by using common multiple access techniques, such as Orthogonal Frequency Division Multiple Access (OFDMA), Code Division Multiple Access (CDMA), and Time Division Multiple Access (TDMA) \cite{WALRAND2000305}. Fig. \ref{fig:system_architecture} gives an overview of this case. We then continue with a more complex scenario, the multi-user case, where we have interfering signal transmissions. Fig. \ref{fig:multi_system_architecture} illustrates the multi-user model.

\begin{figure*}[thp]
	\center
	\begin{subfigure}{0.7\textwidth}
		\includegraphics[width=\linewidth]{figs/multi_system_architecture}
	\end{subfigure}
	\\
	\caption{The overall architecture of our system model in the multi-user scenario. Each UserTX separately encodes binary messages $s_i$s into individual signal vectors denoted as $X_i$ ($X$ is a vector of these vectors). Alice generates a covert signal vector $Z$ for a covert message $m$. Willie receives both normal $Y$ and covert $\hat{Y}$ signals during training, but at the time of operation, performs covert detection on either signal based on covert user activity. Similarly, UserRX extracts the normal messages from either $Y$ or $\hat{Y}$. Bob decodes the covert message from $\hat{Y}$. The colored components are accessible to covert users, while the gray components are inaccessible.}
	\label{fig:multi_system_architecture}
\end{figure*}


\subsection{Single-User Communication}
\textbf{Transmitter}: In the single-user case, the encoder or the transmitter is referred to as UserTX, and the decoder or the receiver is referred to as UserRX. These two entities together form our autoencoder-based normal wireless communication system. The normal communication process begins with UserTX encoding a binary message to a vector of signals using its encoder network. This vector of signals is then transmitted to UserRX and gets distorted while passing through the channel.

\textbf{Channel}: We consider three channel models of AWGN, Rayleigh fading, and Rician fading. To set the \textit{Signal to Noise Ratio} (SNR) in the AWGN model, we keep the transmitter's average power at unit power and adjust the noise power accordingly. For the fading channels, we consider a flat-frequency block-fading channel model, where each signal vector (codeword) is assumed to be faded independently.

\textbf{Receiver}: A noisy version of the transmitted signal is received at the receiver side, where UserRX extracts the message by decoding the signals. In the case of fading channels, the receiver equalizes the signals before passing them to the decoder network.

\textbf{Equalization}: The channel matrix is estimated by UserRX using a blind channel estimation technique, by feeding the received signals to a preliminary network to predict the fading coefficients. Using the estimated channel matrix, UserRX equalizes the signals prior to feeding them to the decoder network. In Fig. \ref{fig:system_architecture} under the \textit{RECEIVERS} section, you can see the two layers of ``Paramter Estimation Layer'' and ``Transformation Layer'' that are responsible for the signal equalization process.

\subsection{Multi-User Communication}
\textbf{Transmitter}: In the multi-user case, multiple transmitters (UserTXs) communicate with a single base station (BaseRX), which serves as the central receiver. Each UserTX uses its own encoder network to encode a binary message to a vector of signals. The transmitters have no knowledge of each other's messages and share no parameters in their networks. Each UserTX and BaseRX pair forms an autoencoder model, with UserTX serving as the encoder and BaseRX as the decoder. After encoding their messages into signals, the transmitters simultaneously send their signals over the channel.

\textbf{Channel}: The multi-user case has the same channel models as described in the single-user case, with the exception that signals experience interference due to simultaneous transmission.

\textbf{Receiver}: BaseRX receives the signals from all the transmitters using multiple antennas after they pass through the channel. It decodes the messages in the same way as the UserRX in the single-user case, but it handles the decoding process for all the transmitters' signals.

\textbf{Equalization}: Similar to the single-user case, BaseRX equalizes the signals before passing them to the decoder networks. However, unlike the single-user receiver, we assume that BaseRX has access to the channel matrix. In practice, BaseRX can use a pilot-based channel estimation technique, which results in a much more accurate estimation of the channel matrix compared to the blind channel estimation used in the single-user case.

\subsection{Covert Communication}
There is a covert sender (Alice) who wants to communicate secretly with her intended receiver (Bob). Bob's existence is no secret to the other entities in the system and anyone, including the observer (Willie), can see his transmissions. However, when Willie suspects that a covert sender like Alice is communicating with Bob, he becomes alerted. Therefore, the primary goal of our covert users is to maintain Alice's communications covert.

\textbf{Alice}: The covert communication starts with Alice using her generative model to embed a confidential message into a covert signal vector. You can find an overview of Alice's network in Figs. \ref{fig:system_architecture} and \ref{fig:multi_system_architecture} under the \textit{TRANSMITTERS} sections. As illustrated in the plots, in non-fading channels, Alice merely needs a covert message and a random trigger to produce a covert signal. However, in fading channels, she requires the channel state information of her channel to Bob in the single-user case, and also the channel state information from other normal users to Bob in the multi-user case. This information will be provided by Bob, which we will explain below. After Alice produces the covert signals, she transmits them into the shared channel irrespective of other users' transmissions. This means that her covert signals should be agnostic to the signals of the normal users.

\textbf{Bob}: Unlike the normal receiver or Willie, Bob only uses a single antenna at his receiver. He receives the covert signals that have undergone channel effects and interference from normal users' transmissions. Without any equalization operation, he uses his decoder network to extract Alice's message from the covert signals. Additionally, he provides Alice with his and other users' channel information. He does first by sending pilot signals to Alice, and second by measuring channel information using the pilot signals sent from normal transmitters to BaseRX and then broadcasting it to Alice. As we mentioned earlier, Bob's existence need not be hidden and him broadcasting this information or pilot signals poses no risk to Alice's secrecy, who is supposed to remain covert.

\textbf{Willie}: Willie listens to all ongoing transmissions on the channel and uses a binary classifier network to determine the likelihood of each signal being covert or normal. Any distortion in the normal signals can alert Willie to the presence of a covert transmission, so Alice must carefully select her covert signals. This means she should not make noticeable changes to the statistical properties of the channel noise or other normal signals. The covert signals should also not increase the system's error rate, as an unexpected increase in the error rate can raise suspicion. We consider Willie to be an integrated module at the receiver of the normal communication system, i.e., UserRX in the single-user and BaseRX in the multi-user case. This way, he can not only detect incoming covert signals but also measure the communication's error rate.

We represent the roles of Alice, Bob, and Willie with DNNs in a training setup similar to GANs. In this adversarial training setting, each of the three roles is encouraged to maximize its performance until they all reach a state of equilibrium at the end of training.

\section{GAN-Based Covert Design}
For a given binary secret message \(m\), Alice first applies a one-hot encoding technique, and then utilizes her generator model to produce a covert signal \(Z\). With this stochastic generative model, each time a different covert signal is generated for the same message. Alice then sends this covert signal to the shared channel, which is accessible to all entities within the system. To simplify notations, we use \(\hat{X}\) to denote the covert signal prior to propagation through the channel.

\begin{equation}
	\hat{X} = X + Z.
\end{equation}

As previously mentioned, we consider three channel models: AWGN, Rayleigh fading, and Rician fading. Therefore, there will be three different channel outputs for these three channel models. We use a mapping function \(\mathcal{C}(\cdot)\) to express each of these channels' outputs. Since signals in the multi-user case also experience interference, we express single-user and multi-user channel's outputs separately.

\textit{AWGN Channel Output}: For the AWGN channel model, the signal received at the receiver carries the channel's noise effect \(N \sim \mathcal{N}(0, \sigma^2)\). Consequently, the channel function \(\mathcal{C}(\cdot)\) and the final covert signal \(\hat{Y}\) in the single-user case can be represented as:
\begin{equation}
	\hat{Y} = \mathcal{C}(\hat{X}) = X + Z + N.
\end{equation}
For the multi-user case the covert signal can be denoted as:
\begin{equation}
	 \hat{Y} = \mathcal{C}(\hat{X}) = \sum_{i \in U}X_i + Z + N.
\end{equation}
where \(U\) is the set that contains all transmitters.

\textit{Rayleigh and Rician Fading Channel Outputs}: In Rayleigh and Rician fading channels, transmitted signals are multiplied by a complex Gaussian random variable with zero mean and a certain variance, which represents the fading effect caused by multipath propagation. Let \(H_{\mathcal{U}}\) be the complex fading coefficient(s) for the normal signal vector(s), and \(H_{\mathcal{A}}\) be the complex fading coefficient for Alice's signal. The channel function \(\mathcal{C}(\cdot)\) and the resulting covert signal \(\hat{Y}\) in the single-user case are given by:
\begin{equation}
	\hat{Y} = \mathcal{C}(\hat{X}) = (H_{\mathcal{U}} \cdot X) + (H_{\mathcal{A}} \cdot Z) + N.
\end{equation}

In the multi-user case, the received covert signal including the channel interference can be written as:
\begin{equation}
	\hat{Y} = \mathcal{C}(\hat{X}) = \sum_{i \in U}(H_{\mathcal{U}_i} \cdot X_i) + (H_{\mathcal{A}} \cdot Z) + N.
\end{equation}

On the receiving end, Bob receives the covert signal \(\hat{Y}\) and uses his decoder network to reconstruct the covert message \(\hat{m}\).

Willie's network is designed to classify sequences of normal signals \(Y\) and covert signals \(\hat{Y}\). In order to ensure that the covert signals remain indistinguishable, Alice must generate them in such a way that Willie's network classifies them as normal signals. To achieve this, Alice requires access to the classifier used by Willie's network. However, this is not feasible in a real-world scenario. Therefore, Alice and Bob perform a black-box attack against Willie's network using a substitute binary classifier. Previous studies have shown that adversarial attacks against one model can be effective on another model, even if the two models have different training sets and architectures \cite{papernot2016transferability}. Willie's substitute network will be used to provide feedback to Alice during training, helping her modify the produced covert signals to make them indistinguishable from normally transmitted signals. This ensures that when the model is deployed in a real communication setup, it is highly unlikely that any observer will detect the covert transmissions.

\subsection{General Formulation}
\textbf{Reliability}: The first objective of our covert model is to enable reliable covert communication. In order to achieve this, Bob needs to be able to accurately decode the covert messages sent by Alice. As mentioned earlier, Alice employs a generative model to produce covert signals that correspond to the covert message. Let \(\Theta_{\mathcal{A}}(\cdot)\) be the underlying function of Alice's generative model that takes a random trigger \(t \sim \mathcal{N}(0, 1)\), a covert message \(m\), the channel coefficients from Alice to Bob \(H_{\mathcal{A}}\), and in the multi-user case, the channel matrix \(H_{\mathcal{U}}\) and produces a covert signal \(Z\). The corresponding covert signal can be denoted as \(Z_{m, t} = \Theta_{\mathcal{A}}(m, t, H_{\mathcal{A}}\footnotemark[1], H_{\mathcal{U}}\footnotemark[2])\). Let  \(\Theta_{\mathcal{B}}(\cdot)\) be the underlying function of the decoder network that Bob uses to reconstruct the covert message \(\hat{m}\). Then the reliability of communication between Alice and Bob is achieved using the following loss function:
\begin{equation}
	\begin{aligned} \label{bob_loss}
	\mathcal{L}_{\mathcal{B}} & = \mathbb{E}_{m}[\mathcal{H}(\hat{m}, m)] \\
	& = \mathbb{E}_{m}[\mathcal{H}(\Theta_{\mathcal{B}}(\hat{Y}), m)] \\ 
	& = \mathbb{E}_{m}[\mathcal{H}(\Theta_{\mathcal{B}}(\mathcal{C}(\hat{X})), m)] \\ 
	& = \mathbb{E}_{m}[\mathcal{H}(\Theta_{\mathcal{B}}(\mathcal{C}(\Theta_{\mathcal{A}}(m, t, H_{\mathcal{A}}) + X)), m)].
	\end{aligned}
\end{equation}
For the multi-user case, this equation is written as:
\begin{equation}
	\begin{aligned}
		\mathcal{L}_{\mathcal{B}} & = \mathbb{E}_{m}[\mathcal{H}(\hat{m}, m)] \\
		& = \mathbb{E}_{m}[\mathcal{H}(\Theta_{\mathcal{B}}(\mathcal{C}(\Theta_{\mathcal{A}}(m, t, H_{\mathcal{A}}, H_{\mathcal{U}}) + X)), m)].
	\end{aligned}
\end{equation}
The equation above uses the cross-entropy function \(\mathcal{H}(\cdot)\) to measure the difference between the probability distribution of the reconstructed covert message \(\hat{m}\) and that of the actual covert message \(m\). This equation can be used to optimize the networks of both Alice and Bob by freezing one or the other's network parameters iteratively.

\footnotetext[1]{Parameter for fading channel only.}
\footnotetext[2]{Parameter used only in multi-user fading channel.}

\textbf{Low Interference}: While (\ref{bob_loss}) ensures communication accuracy, we also need to ensure that the generated perturbations do not negatively impact normal communication between UserTX and UserRX. Otherwise, this could alert Willie to abnormal activity. To address this, we add a constraint that minimizes UserRX's loss function during Alice's training. In a single-user system, we can express this constraint as follows:
\begin{equation}
	\begin{aligned} \label{alice_user_loss}
	\mathcal{L}_{\mathcal{U}} & = \mathbb{E}_{m}[\mathcal{H}(\hat{s}, s)] \\
	& = \mathbb{E}_{m}[\mathcal{H}(\Theta_{\mathcal{U}}(\hat{Y}), s)] \\
	& = \mathbb{E}_{m}[\mathcal{H}(\Theta_{\mathcal{U}}(\mathcal{C}(\hat{X})), s)] \\
	& = \mathbb{E}_{m}[\mathcal{H}(\Theta_{\mathcal{U}}(\mathcal{C}(\Theta_{\mathcal{A}}(m, t, H_{\mathcal{A}}) + X)), s)].
	\end{aligned}
\end{equation}
where \(\Theta_{\mathcal{U}}(\cdot)\) refers to UserRX's decoder network function. It is important to note that during this training, UserRX's decoder network is frozen and only Alice's parameters will be updated.

For the multi-user case, since we have multiple transmitters sending signals, we need to minimize BaseRX's loss function over all individual transmitters' signals. Thus, (\ref{alice_user_loss}) is rewritten as follows:
\begin{equation}
	\begin{aligned} \label{multi_alice_user_loss}
		\mathcal{L}_{\mathcal{U}} & = \sum_{i \in U}\mathbb{E}_{m}[\mathcal{H}(\hat{s}_i, s_i)] \\
		= \sum_{i \in U} & 
			\mathbb{E}_{m}[\mathcal{H}
			(\Theta_{\mathcal{U}}(\mathcal{C}(\Theta_{\mathcal{A}}(m, t, H_{\mathcal{A}}, H_{\mathcal{U}}) + X),  H_{\mathcal{U}}), s_i)].
	\end{aligned}
\end{equation}

\begin{algorithm}[tp!]
	\caption{Training covert models algorithm}\label{alg:train}
	\small
	\begin{algorithmic}
		\State $X \gets$ normal signals data
		\State $S, M \gets$ normal and covert messages sets
		\State $\Theta_{\mathcal{A}}, \Theta_{\mathcal{B}}, \Theta_{\mathcal{W}} \gets$ Alice, Bob, and Willie networks
		\State $\Theta_{\mathcal{U}} \gets$ UserRX decoder network
		\State $\mathcal{H} \gets$ cross entropy 
		\State $\mathcal{C} \gets$ channel mapping function
		\For{epoch $ep \in \{1 \ldots n_{epochs}$\}}
		\State $t \sim \mathcal{N}(0, 1)$
		\State $\mathcal{L}_{\mathcal{W}} = \mathcal{H}(\mathcal{C}(X), \mathcal{C}(\Theta_{\mathcal{A}}(M, t, H_{\mathcal{A}}, H_{\mathcal{U}}) + X))$
		\State Update $\Theta_{\mathcal{W}}$ to minimize $\mathcal{L}_{\mathcal{W}}$
		\State $\mathcal{L}_{\mathcal{B}} = \mathcal{H}(\mathcal{C}(\Theta_{\mathcal{A}}(M, t, H_{\mathcal{A}}, H_{\mathcal{U}}) + X), M)$
		\State Update $\Theta_{\mathcal{B}}$ to minimize $\mathcal{L}_{\mathcal{B}}$
		\State $\mathcal{L}_{\mathcal{U}} \gets \mathcal{H}(\Theta_{\mathcal{U}}(\mathcal{C}(\Theta_{\mathcal{A}}(M, t, H_{\mathcal{A}}, H_{\mathcal{U}}) + X)), S)$
		\State
		$\mathcal{L}_{\mathcal{A}} = \lambda_{\mathcal{B}} \mathcal{L}_{\mathcal{B}} + \lambda_{\mathcal{U}} \mathcal{L}_{\mathcal{U}} - \lambda_{\mathcal{W}} \mathcal{L}_{\mathcal{W}}$
		\State Update $\Theta_{\mathcal{A}}$ to minimize $\mathcal{L}_{\mathcal{A}}$
		\EndFor
	\end{algorithmic}
\end{algorithm}

\textbf{Undetectability}: In our model, Willie acts as a critic network, similar to what is known as a discriminator in GAN models \cite{goodfellow2014generative}. The so-called real and fake samples in a GANs' discriminator training process here are mapped to normal and covert signals, respectively. For the single-user case, we use the following loss function to optimize Willie's network that classifies covert and normal signals:
\begin{equation}
	\begin{aligned} \label{willie_loss}
	\mathcal{L}_{\mathcal{W}} & = \mathbb{E}_{m}[\mathcal{H}(\hat{Y}, Y)] \\
	& = \mathbb{E}_{m}[\mathcal{H}(\mathcal{C}(\hat{X}), \mathcal{C}(X))] \\
	& = \mathbb{E}_{m}[\mathcal{H}(\mathcal{C}(\Theta_{\mathcal{A}}(m, t, H_{\mathcal{A}}) + X), \mathcal{C}(X))].
	\end{aligned}
\end{equation}
where \(\mathcal{H}(\cdot)\) is the binary cross entropy between the predicted outputs for the covert signals \(\hat{Y}\) or the normal signals \(Y\) and their actual outputs. For the multi-user case, we need to optimize Willie's network over all the transmitters' outputs.

\begin{equation}
	\begin{aligned} \label{multi_willie_loss}
		\mathcal{L}_{\mathcal{W}} & = 
		\sum_{i \in U} \mathbb{E}_{m}[H(\hat{Y}, Y)] \\
		& = \sum_{i \in U}
			\mathbb{E}_{m}[\mathcal{H}(\mathcal{C}(\Theta_{\mathcal{A}}(m, t, H_{\mathcal{A}}, H_{\mathcal{U}}) + X), \mathcal{C}(X))].
	\end{aligned}
\end{equation}

This white-box adversarial training against Alice's network ensures that Willie's network will be adequately trained to distinguish between covert and normal signals. On the other hand, we do not want the covert signals produced by Alice to deviate from the statistical properties of the normal signals on the channel, as this would increase the likelihood of the observer detecting and mitigating the covert communication. To achieve this undetectability property, we use Willie's network to act as a discriminator network in Alice's optimization function. In other words, Alice's training against this network aims to maximize Willie's uncertainty about his predictions. This regularizer helps Alice form their covert communication in a way that is indistinguishable from the actual channel's noise, yet understandable by Bob. Overall, Alice's loss function can be expressed as a weighted sum of these three objectives:
\begin{equation}
	\begin{array}{l} \label{alice_loss}
	\mathcal{L}_{\mathcal{A}} = \lambda_{\mathcal{B}} \mathcal{L}_{\mathcal{B}} + \lambda_{\mathcal{U}} \mathcal{L}_{\mathcal{U}} - \lambda_{\mathcal{W}} \mathcal{L}_{\mathcal{W}}.
\end{array}
\end{equation}
where \(\lambda_{\mathcal{B}}\), \(\lambda_{\mathcal{U}}\), and \(\lambda_{\mathcal{W}}\) are hyperparameters that determine the relative importance of the different objectives in Alice's loss function. The algorithmic steps involved in training our covert models are summarized in Algorithm \ref{alg:train}.


\subsection{Neural Network Architecture}
\textbf{User's Autoencoder Network}: The autoencoder model takes in a binary message \(s\) of size \(k\) bits and outputs a reconstructed version of it, denoted by \(\hat{s}\). The encoder maps the message to a vector of signals of size \(2 \times n\), where \(n\) is the number of channel uses, using one-hot encoding. Since there are multiple encoders in the multi-user case, the resulting vector will have a size of \(n_{tx} \times (2 \times n)\). The signals generated from the encoder then pass through the channel, where the channel mapping function applies the channel's effects. When the system is single-user and there is fading in the channel model, the receiver side utilizes the two layers of parameter estimation and transformation to equalize the signals. We apply a simple transformation function that divides signals by the channel fading coefficients estimated via the parameter estimation layer. Note that more complex transformation functions can be used, as described in \cite{o2017introduction}; however, optimizing the performance of the autoencoder model is out of the scope of this article. For the multi-user case, the decoder is provided with channel coefficients as input. This enables BaseRX to use the zero-forcing technique \cite{garg2010wireless} to equalize the received signals. In the case of a single-user system, UserRX utilizes its decoder network to reconstruct the message. However, in the multi-user case, BaseRX receives signals from all transmitters and simultaneously decodes them. This is achieved by first passing the signals to a pre-decoder network and then using separate decoders at the final layers.


\textbf{Alice's Network}: Alice first transforms a covert message \(m\) to its corresponding one-hot encoding representation, where each message belongs to a unique class. She then uses a random trigger \(t\) to randomize the process by which the covert noise signal \(Z\) is produced, along with the channel coefficients \(H_{\mathcal{A}}\) and \(H_{\mathcal{U}}\). For Alice's generator model, we use multiple dense layers with ReLU and Tanh activation functions. The first layer acts as an embedding layer, enlarging the input's domain space. The subsequent fully connected layers extract useful features and perform the encoding process. Finally, the last layer performs a dimension transformation, ensuring that the generated covert signal \(Z\) complies with the dimension of the normal signal \(X\) on the channel. 


\textbf{Bob's Network}: Bob receives the covert signal \(\hat{Y}\) that has been affected by the channel, and he feeds it through his decoder network to extract the secret message. Bob's network is more sophisticated than Alice's, as decoding such a distorted signal is a much more complex task. The received message first goes through a wide dense layer with a Tanh activation function, which increases the input's feature space. The data then passes through multiple 1-Dimensional Convolutional (1D Conv) layers, which learn the coding that Alice has developed to encode the covert messages. We have found that using 1D Conv layers helps Bob and Alice achieve better consistency in the accuracy of their communication, especially when the channel model is more complicated (i.e., when there is also fading in the channel). The rest of Bob's decoder network consists of two dense layers that remap the learned feature space to the covert message domain space. As with UserRX's and BaseRX's decoder networks, Bob eventually predicts the covert message by performing a classification on the received signal.

\textbf{Willie's Network}: Willie's task is to distinguish between the normal signal  \(Y\) and the covert  signal \(\hat{Y}\). To achieve this, he uses a neural network with the same architecture as Bob's, except for the last layer, which has a Sigmoid activation function instead of Softmax. The network takes an input signal, either normal or covert, and produces a confidence probability \(P\) indicating the likelihood of the signal being normal. Using the same network architecture for both Bob and Willie ensures a fair competition between them, as they have the same capacity for learning.

\begin{figure*}[!tp]
	\center
	\begin{subfigure}{0.28\textwidth}
		\includegraphics[width=\linewidth]{figs/autoencoder_bler_awgn}
		\caption{AWGN channel}
	\end{subfigure}
	\begin{subfigure}{0.28\textwidth}
		\includegraphics[width=\linewidth]{figs/autoencoder_bler_rician}
		\caption{Rician fading channel}	
	\end{subfigure}
	\begin{subfigure}{0.28\textwidth}
		\includegraphics[width=\linewidth]{figs/autoencoder_bler_rayleigh}
		\caption{Rayleigh fading channel}	
	\end{subfigure}
	\caption{Autoencoders' performance in terms of BLER over a range of SNR values is evaluated in our single-user case. The models are trained over AWGN, Rayleigh, and Rician fading channels for a set of parameters that have the same data rate.}
	\label{fig:autoencoder_bler}
\end{figure*}
