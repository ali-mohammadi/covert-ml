\section{Our Covert Communication System Model}
\label{s:model}
In this section, we begin with an overview of our covert communication model and will further discuss the details of our scheme in the subsequent sections.
\subsection{Overview}
An overview of our system architecture is shown in figure (\ref{fig:system_architecture}). The main idea of our work is to establish a covert channel on top of a normal communication between a sender (UserTX) and a receiver (UserRX) who are using an autoencoder-based wireless system for their communications. We consider both an AWGN and a Rayleigh fading channel between our communication parties. The objective of the covert sender (Alice) is to secretly communicate with the covert receiver (Bob) by embedding messages in form of perturbations that have similar statistical properties as of the channel's noise. This is achieved by help of an observer (Willie) who tries to rigorously classify transmitted signals as covert and non-covert after they pass through the channel.\\
\subsection{System Architecture}
As mentioned above, there are three main actors in our covert system which we call them by their placeholder names: Alice, Bob, and Willie. All three are collaborating in our covert scheme and are represented by DNNs. Alice uses a generative model that embeds a confidential message \(m\) into a covert noise vector \(\hat{z}\). This covert signal is then transmitted over the channel after being added to a normal signal \(x\). Similar to every covert communication scheme, there is an observer or warden in the system that will be alerted when seeing any deviation in the statistical properties of the channel. To this end, we are implicitly incorporating a statistical undetectability constraint on the produced covert signals by having a discriminator network used by Willie. The presence of this discriminator network helps us to ensure that these added covert noise signals are distributed as the channel's real noise, making them undetectable.\\
For a given binary secret message \(m\), Alice first one-hot encodes the message and then uses its generator model to produce a covert noise signal \(\hat{z}\). This covert signal is then added to a vector of a normal signal, which is carrying a message between UserTX and UserRX. Therefore, the covert signals before being transmitted over the channel can be denoted as:
\begin{equation}
	\hat{x} = x + \hat{z}
\end{equation}
The signal is then transmitted over the channel. We mentioned that we assume the channel between sender and receiver to be AWGN or Rayleigh. Therefore, there will be two different channel outputs for these two different channel models. We express the distortions caused by the channel as a mapping function \(C(\cdot)\).
\subsubsection{AWGN Channel Output}
For the AWGN channel model, the signal received at the receiver carries within itself the channel noise effect \(z \sim \mathcal{N}(0, \sigma_{chl}^2)\). Thus, the channel function \(C(\cdot)\) and final covert signal \(\hat{y}\) can be represented as:
\begin{equation}
	C(\hat{x}) \Rightarrow \hat{y} = \hat{x} + z
\end{equation}

\subsubsection{Rayleigh Fading Channel Output}
For the Rayleigh fading channel model, we consider a flat block fading channel where each codeword is assumed to be faded independently. Let \(h\) be the fading coefficient for transmitting the codeword \(\hat{x}\), then the channel function \(C(.)\) and and the final covert signal \(\hat{y}\) is given by:
\begin{equation}
	C(\hat{x}) \Rightarrow \hat{y} = h \cdot \hat{x} + z
\end{equation}
On the receiver side, Bob receives a transmitted signal \(\hat{y}\) after the channel noise is added into it. He uses its decoder network to reconstruct the covert message \(\hat{m}\). Meanwhile, the UserRX is using the same signal to extract the normal message \(\hat{s}\), which is the reconstructed message of \(s\). The statistical properties of signals on the channel are captured by Willie who observes the channel continuously. His objective is to classify sequences of normal \(y\) and covert signals \(\hat{y}\) that he sees on the channel and provide useful feedback to Alice. This feedback helps Alice to modify the produced covert signals such that they are indistinguishable from normal transmitted signals. In other words, it ensures that both normal and covert signals have the same statistical properties.
\subsection{General Formulation}
The very first objective of our covert model is to have a working covert channel. To this end, Bob has to have a plausible accuracy in decoding covert messages that Alice sends through the covert signals \(\hat{y}\). As mentioned in previous section, Alice employs a generative model instead of an encoder model suggested by \cite{mohammed2021adversarial}. Using an encoder model to produce covert signal perturbations will map each covert message \(m\) to a single covert noise vector \(\hat{z}\). Inevitably, these deterministic covert perturbations can be detected and averaged out with ease by a careful observer or a defender as already shown in a work of Bahramali et al. \cite{bahramali2021robust} studying a relatable covert attack problem against autoencoder wireless networks. Thus, we use an stochastic generative model for Alice so that each covert message gets mapped to a set of different covert noise signals. Let \(A(\cdot)\) be the underlying function of Alice's generative model that takes a random trigger \(t \sim \mathcal{N}(0, 1)\) and a covert message \(m\) and produces a covert signal \(\hat{z}\) (the corresponding covert signal then can be denoted as \(\hat{z}_{m, t} = A(m, t)\)). Let also  \(B(\cdot)\) be the underlying function of the decoder network that Bob makes use of to reconstruct the covert message \(\hat{m}\). Then the reliability of communication between Alice and Bob is achieved using the below loss function:
\begin{equation}
	\begin{array}{l} \label{bob_loss}
	\mathcal{L}_{Bob} = \mathbb{E}_{m}[CE(\hat{m}, m)] \\ \Rightarrow \mathbb{E}_{m}[CE(B(C(A(m, t) + x)), m)]
	\end{array}
\end{equation}
where \(CE(\cdot)\) is the cross entropy between the reconstructed covert message \(\hat{m}\) and the actual covert message \(m\). This equation can be used to optimize both for Alice's and Bob's networks by freezing one or the other network's parameters iteratively. While (\ref{bob_loss}) ensures the communication accuracy, we also need to bear in mind that the generated perturbations should have no detrimental impact on the normal communication between UserTX and UserRX, otherwise an unexpected increase in the error rate of communication can be a sign of an abnormal behavior. We apply this constraint by minimizing the autoencoder's loss function during Alice's training:
\begin{equation}
	\begin{array}{l} \label{alice_user_loss}
	\mathcal{L}_{UserRX} = \mathbb{E}_{m}[CE(\hat{s}, s)] \\ \Rightarrow \mathbb{E}_{m}[CE(D(C(A(m, t) + E(s))), s)]
	\end{array}
\end{equation}
where \(D(\cdot)\) is UserRX's decoder network function, and \(E(\cdot)\) is the underlying function of the UserTX's encoder network. Note that both UserTX's encoder and UserRX's decoder networks are frozen during this training and only Alice's parameters are updated.\\
In our model, the observer entity or Willie, acts as the discriminator in GAN models \cite{goodfellow2014generative}. The so-called real and fake data in GANs' discriminator training here is mapped to non-covert and covert signals and we define the loss function for Willie as:
\begin{equation}
		\begin{array}{l} \label{willie_loss}
	\mathcal{L}_{Willie} = \mathbb{E}_{m}[BCE(\hat{y}, y)] \\ \Rightarrow \mathbb{E}_{m}[BCE(C(A(m,t) + x), C(x))]
	\end{array}
\end{equation}
where \(BCE(\cdot)\) is the binary cross entropy between the covert signal \(\hat{y}\) and the normal signal \(y\). This white-box adversarial training against Alice's network ensures that Willie will be adequately trained to tell covert and non-covert signals apart. On the other hand, we do not want the covert signals that Alice produces to deviate from the statistical properties of the normal signals on the channel, otherwise it is likely that the observer of the channel detects and mitigates the covert communication. To achieve this undetectability property, we pose a new constraint on Alice's optimization function for maximizing Willie's uncertainty about his predictions. Having a regularizer as such helps Alice and Bob to form their covert communication in a way that is indistinguishable from the actual channel's noise, yet understandable by both. Altogether, Alice's loss function can be expressed as a weighted sum of three different objectives:
\begin{equation}
	\begin{array}{l} \label{alice_loss}
	\mathcal{L}_{Alice} = \lambda_{Bob} \mathcal{L}_{Bob} + \lambda_{UserRX} \mathcal{L}_{UserRX} - \lambda_{Willie} \mathcal{L}_{Willie}
\end{array}
\end{equation}
where \(\lambda_{Bob}\), \(\lambda_{UserRX}\), and \(\lambda_{Willie}\) determine the importance of each objective for training Alice's network.
\iffalse Algorithm (?) summarizes our approach for optimizing the whole system. \fi
\begin{table}[bp!]
	\begin{adjustbox}{width=0.85\columnwidth,center}
		\begin{tabular}{|l|l|} 
			\hline
			\multicolumn{2}{|c|}{\textbf{Encoder}} 															\\
			\hline
			Layer 																	&	Output dimension	\\
			\hline
			Input (size 16)      												&	-    	 		    \\ 
			Dense + ELU          													&	16					\\
			Dense + ELU   															&	2 $\times$ 8		\\
			Convolutional (8 filters, kernel size 1 $\times$ 2, stride 1) + Tanh 	&   8 $\times$ 15		\\
			Convolutional (8 filters, kernel size 1 $\times$ 4, stride 2) + Tanh 	&   8 $\times$ 6		\\
			Convolutional (8 filters, kernel size 1 $\times$ 2, stride 1) + Tanh 	&   8 $\times$ 5		\\
			Convolutional (8 filters, kernel size 1 $\times$ 2, stride 1) + Tanh 	&   8 $\times$ 4		\\
			Flatten															 		&   32					\\
			Dense																	&	2 $\times$ 8		\\
			Normalization															&	2 $\times$ 8		\\
			\hline   
			\hline												
			\multicolumn{2}{|c|}{\textbf{Parameter Estimation}} 											\\
			\hline
			Dense + ELU																&	2 $\times$ 16		\\
			Dense + Tanh															&	2 $\times$ 32		\\
			Dense + Tanh															&	2 $\times$ 8		\\
			Dense																	&	2 $\times$	1		\\
			\hline
			\hline
			\multicolumn{2}{|c|}{\textbf{Decoder}}															\\
			\hline
			Layer 																	&	output dimension	\\
			\hline
			Dense + Tanh          													&	2 $\times$ 8		\\
			Convolutional (8 filters, kernel size 1 $\times$ 2, stride 1) + Tanh 	&   8 $\times$ 15		\\
			Convolutional (8 filters, kernel size 1 $\times$ 4, stride 2) + Tanh 	&   8 $\times$ 6		\\
			Convolutional (8 filters, kernel size 1 $\times$ 2, stride 1) + Tanh 	&   8 $\times$ 5		\\
			Convolutional (8 filters, kernel size 1 $\times$ 2, stride 1) + Tanh 	&   8 $\times$ 4		\\
			Flatten															 		&   32					\\
			Dense + Tanh															&	2 $\times$ 8		\\
			Dense + Tanh															&	2 $\times$ 8		\\
			Dense + Softmax															&	16					\\ 
			\hline
		\end{tabular}
	\end{adjustbox}
	\caption{Autoencoder's network detailed architecture}
	\label{table:autoencoder_structure}
\end{table}
\begin{table}[tph!]
	\begin{adjustbox}{width=0.85\columnwidth,center}
		\begin{tabular}{|l|l|} 
			\hline
			\multicolumn{2}{|c|}{\textbf{Alice}} 															\\
			\hline
			Layer 																	&	Output dimension	\\
			\hline
			Input (size 8 + $2^k$)      											&	-    	 		    \\ 
			Dense + ReLU          													&	32 + $2^{k+1}$		\\
			Dense + ReLU          													&	32 + $2^{k+1}$		\\
			Dense + ReLU   															&	8 $\times$ $2^k$	\\
			Dense																	&	8 $\times$ 2	\\
			\hline   
			\hline												
			\multicolumn{2}{|c|}{\textbf{Bob, Willie}} 											\\
			\hline
			Input (size 2 $\times$ 8)
			Dense + Tanh																&	2 $\times$ 8			\\
			Convolutional (8 filters, kernel size 1 $\times$ 1, stride 1) + LeakyReLU 	&   8 $\times$ 16			\\
			Convolutional (8 filters, kernel size 1 $\times$ 2, stride 1) + LeakyReLU 	&   8 $\times$ 15			\\
			Convolutional (8 filters, kernel size 1 $\times$ 4, stride 2) + LeakyReLU 	&   8 $\times$ 6			\\
			Convolutional (8 filters, kernel size 1 $\times$ 2, stride 1) + LeakyReLU 	&   8 $\times$ 5			\\
			Convolutional (8 filters, kernel size 1 $\times$ 2, stride 1) + LeakyReLU 	&   8 $\times$ 4			\\
			Flatten															 			&   32						\\
			Dense + Tanh																&	16						\\
			Dense + (Willie: Sigmoid, Bob: Softmax)										&	Willie: 1, Bob:	$2^k$	\\
			\hline
		\end{tabular}
	\end{adjustbox}
	\caption{Alice, Bob, and Willie's networks detailed architecture}
	\label{table:covert_models_structure}
\end{table}
\subsection{Neural Network Architecture}
Before discussing the architecture of our neural network models, we need to state the focus of this work is not to introduce an autoencoder wireless network, so we only give a brief description on how this model works and a more detailed explanation of such a network and the training procedure can be found in the original paper \cite{o2017introduction}. An overview of the implemented autoencoder network's architecture can be found in table (\ref{table:autoencoder_structure}). Likewise, table (\ref{table:covert_models_structure}) presents details of our covert models.\\
Similar to what is proposed in the original autoencoder wireless communication paper, our autoencoder model accepts a binary message \(s\) of size \(k\) bits. The encoder part of the model first one-hot encodes the message in order to represent each message as a different class. Then, using its encoder network the message is mapped to a vector of signals of size \(2 \times n\), where \(n\) is the number of channel uses. This transmitted signal is then given to a mapping function that applies the channel effects into. On the receiver side, there will be two different structures given what the channel model is. In case of an AWGN channel model, the signal is simply passed through the decoder network and the intended message gets extracted. For the Rayleigh fading channel model, however, there will be a parameter estimation model that takes in the signal before it passes through the decoder network and estimates the channel's fading coefficients. Followed by this operation, the extracted signal along with the estimated parameters are passed to a transformation function that is supposed to revert the channel effects. In our case, we are using a simple division transformation function that divides the received signal by the estimated channel fading coefficients. Note that more complex transformation functions can be used and are described in \cite{o2017introduction}, however optimizing the performance of autoencoder model is out of the scope of this article. Eventually, the transformed signal is fed to the decoder's network and the original normal message is reconstructed by classifying the signal. Similar to the encoder network, Alice takes a covert message \(m\) and transforms it to its corresponding one-hot encoding representation of it so that each message belongs to a unique class. Next, given a random trigger \(t\), Alice uses its generator model to produce a covert noise signal \(\hat{z}\) and then adds it to a normal signal \(x\) that is being transmitted at the time. Bob receives this covert signal \(\hat{y}\) that has undergone the channel's effects and feeds it through its decoder network regardless of what the channel model is and extracts the secret message by doing classification on the signal. Meanwhile, Willie receives both the covert signal \(\hat{y}\) and the normal signal \(y\) and outputs a confidence probability \(P\) on how probable it is for the signal to be normal.
For the Alice's generator model, we use multiple dense layers with ReLU and Tanh activation functions. The first layer of this model takes a trigger number \(t\) and an one-hot encoded covert message \(m\), and acts as embedding layer by enlarging the input's domain space. The following fully contacted layers are to extract the useful features and do the encoding process. The last layer of this model does a dimension transformation so that the generated covert signal \(\hat{z}\) complies with the dimension of the normal signal \(x\) on the channel. Bob's network has a more complicated structure comparing to Alice as it has to decode the secret message from a signal \(\hat{y}\) that has been distorted stiffly as a result of going through the channel. The received message by Bob first goes through the first layer of the network, which is a wide dense layer with a Tanh activation function, to increase the input's feature space. Then the data is passed through multiple 1-Dimensional Convolutional (1D Conv) layers that supposedly learn the coding that Alice has fabricated to encode the covert messages. We have found that using 1D Conv layers helps Bob and Alice achieving a better consistency in the accuracy of their communication, especially when the channel model is more complicated (i.e. when there is also fading in the channel). The rest of Bob's decoder network consists of two dense layers that does a domain remapping from the learned feature space to the covert message domain space. Similar to the UserRX's decoder network, Bob eventually predicts the covert message by doing a classification on the received signal.\\
We choose the same network architecture of Bob's for Willie except for the last layer that has a Sigmoid activation function instead of Softmax. This ensures that Bob and Willie has the same capacity of training and can be compete each other in a fair setup.