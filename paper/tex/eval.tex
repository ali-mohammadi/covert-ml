\section{Experiments and Evaluation}
\label{s:eval}
Before implementing the covert models, we implemented an autoencoder communication network for the normal communication between UserRX and UserTX. The functionality and architectural design of this model is already described in the previous section. Based on the notation used in \cite{o2017introduction}, an \(Autoencoder (n, k)\) is a neural network communication model that sends \(k\) bits of data in \(n\) channel uses. We chose these two numbers of channel uses \(n\) and the binary message of size \(k\) to be 8 and 4, respectively. These numbers are chosen this way so that we could evaluate the performance of our trained autoencoder model with the results given on \cite{o2017introduction}. Nevertheless, our covert model works independent of these parameters and can be used for any autoencoder communication setup. For training the autoencoder mode, we generate two datasets of train and test by generating random binary messages \(s\) of size \(k\). The training set contains 8192 random binary messages while this number is 51200 for the test set. We intentionally created a much larger data set for testing to make sure that each symbol \(y\) undergoes various channel distortions while evaluating the model's performance. We set the learning rate to 0.001 and optimized the model using the Adam optimizer \cite{kingma2014adam}. We chose the batch size to be 16 and trained the model for 100 epochs. For the channel configuration, we chose fixed signal to noise ratio (SNR) values during the training. The SNR value for the AWGN channel is set to 4dB, and we give a higher SNR value of 16dB to the Rayleigh fading channel due to the channel complexity. Figure (\ref{fig:autoencoder_bler}) shows the performance of the trained normal communication model in terms of block error rate (BLER) for a range of SNR values under AWGN and Rayleigh fading channel conditions.\\
\begin{figure}[bp!] \label{fig:traning_progress}
	\begin{subfigure}{0.235\textwidth}
		\includegraphics[width=\linewidth]{figs/training_progress_awgn}
		\caption{AWGN channel}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}{0.235\textwidth}
		\includegraphics[width=\linewidth]{figs/training_progress_rayleigh}
		\caption{Rayleigh fading channel}	
	\end{subfigure}
	\caption{Covert models' accuracy evaluation during training progress}%
\end{figure}
For the covert models, we evaluated the system on two different channel models of AWGN and Rayleigh fading. In both settings, we used the same training procedure and network architecture for our covert models. We started our experiment by sending 1 bit of covert data over 8 channel uses and gradually increased the number of covert bits to see how increasing the covert data rate would effect each component of our covert scheme. Each covert message can be sent along with a normal message; hence, we generated train and test covert messages \(m\) to have the same number of train and test messages as of the autoencoder's. All models are jointly trained for 5000 epochs using Adam optimizer with the learning rate of 0.0001. In each epoch, we first update parameters of Willie's network using (\ref{willie_loss}), then train Alice's network for one step using (\ref{alice_loss}), and eventually optimize Bob's network based on (\ref{bob_loss}). Although we trained the autoencoder network on a fixed SNR value, we found when our covert scheme is trained on a range of SNR values, not only Alice learns to preserve the normal communication's accuracy better but also Bob will be able to decode the covert messages more accurately. Accordingly, we set the SNR value to be in the range of -2dB to 2dB for the AWGN channel and 5dB to 10dB for the Rayleigh fading channel.
\begin{figure*}[th!] \label{fig:awgn_results}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\linewidth]{figs/covert_autoencoder_bler_awgn}
		\caption{Autoencoder BLER before and after covert transmission}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\linewidth]{figs/bob_bler_awgn}
		\caption{Bob's BLER}	
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\linewidth]{figs/willie_accuracy_awgn}
		\caption{Willie's accuracy}	
	\end{subfigure}
	\caption{Covert models performance after successful training for AWGN channel}%
\end{figure*}
\begin{figure*}[th!] \label{fig:rayleigh_resutls}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\linewidth]{figs/covert_autoencoder_bler_rayleigh}
		\caption{Autoencoder BLER before and after covert transmission}
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\linewidth]{figs/bob_bler_rayleigh}
		\caption{Bob's BLER}	
	\end{subfigure}
	\hspace*{\fill}
	\begin{subfigure}{0.3\textwidth}
		\includegraphics[width=\linewidth]{figs/willie_accuracy_rayleigh}
		\caption{Willie's accuracy}	
	\end{subfigure}
	\caption{Covert models performance after successful training for Rayleigh fading channel}%
\end{figure*}
In figure (?), you can see the results of our training progress. Figure (?) shows the progress of each covert actor's accuracy for the test set during training process. As the training goes on, Bob gradually learns to decode covert messages \(m\) and establishes a reliable communication with Alice. After a few epochs, when the covert communication begins to take up, signals start to deviate from the distribution they had, causing Willie to better detect covert signals. When Willie's accuracy increases, the term \(\mathcal{L}_{Willie}\) dominates the other two objectives of Alice's loss function in (\ref{alice_loss}). This causes Alice to gradually sacrifice its accuracy for the sake of undetectability. Soon afterwards, the training process reaches a stable point where neither of covert models sees any noticeable improvement in their performance as the training continues. Figure (?) depicts the accuracy of Willie in percentage over a range of SNR values for flagging signals as covert and non-covert. This plot tells us how probable it is for our covert signal to be detected by a target detector at a specific SNR value. Figure (?) shows how each entity's accuracy of our covert model changes as we increase the covert rate. Expectedly, the covert communication becomes more unreliable, the impact on the normal communication increases, and detection becomes easier as the covert rate increase.\\
Figure (?) compares the constellation cloud of a covert and non-covert signal for both AWGN and Rayeligh fading channels. We have marked each symbol of the encoder's ouput signal \(x\) as blue points on the constellation diagrams. Red constellation cloud shows how covert signals scatters after going through the channel and the green cloud is for the normal signals passed through the channel. Since data is sent over 8 channel uses, there are 8 blue points on the chart. To be consistent with Willie's accuracy and Bob's error rate for both channel models, we have set the SNR value to 2dB for the AWGN and to 10dB for the Rayleigh fading channel so that in both channels, the probability of detection remains relatively the same and covert communication BLER stays around \(10^{-1}\). This area of operation gives Alice and Bob a fair covert communication reliability while being able to maintain their covertness. As it is also visible in the constellation diagrams, Alice has learned to perfectly cloak its covert signals into the distribution of the channel effects.