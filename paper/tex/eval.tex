\section{Experiments and Evaluation}
\label{s:eval}
Before implementing the covert models, we implemented an autoencoder communication network for the normal communication between UserRX and UserTX. The functionality and architectural design of this model is already described in the previous section. Based on the notation used in \cite{o2017introduction}, an Autoencoder \((n, k)\) is a neural network communication model that sends \(k\) bits of data in \(n\) channel uses. We chose these two numbers of channel uses \(n\) and the binary message of size \(k\) to be 8 and 4, respectively. These numbers are chosen this way so that we could evaluate the performance of our trained autoencoder model with the results given on \cite{o2017introduction}. Nevertheless, our covert model works independent of these parameters and can be used for any autoencoder communication setup. For training the autoencoder mode, we generate two datasets of train and test by generating random binary messages of size \(k\). The training set contains 8192 random binary messages while this number is 51200 for the test set. We intentionally created a much larger data set for testing to make sure that each symbol undergoes various channel distortions while evaluating the model's performance. We set the learning rate to 0.001 and train the model for 100 epochs. Figure () shows the performance of the normal communication model in terms of block error rate (BLER) for a range of SNR values for AWGN and Rayleigh fading channel.\\
For the covert models, we evaluated the system on two different channel models of AWGN and Rayleigh fading. In both settings, we used the same training procedure and network architecture for our covert models. We started our experiments by sending 1 bit of covert data and gradually increased the data rate to see how increasing the covert data rate would effect each component of the model. With knowing the fact that our covert model has to embody its covert noise signal to a normal signal generated by the autoencoder communication model, we generated randomly the same number of train and test binary messages as of the autoencoder's for the covert message data sets.