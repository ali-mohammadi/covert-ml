\section{Experiments and Evaluation}
\label{s:eval}
Before implementing the covert models, we implemented an autoencoder communication network for the normal communication between UserRX and UserTX. The functionality and architectural design of this model is already described in the previous section. Based on the notation used in \cite{o2017introduction}, an Autoencoder \((n, k)\) is a neural network communication model that sends \(k\) bits of data in \(n\) channel uses. We chose these two numbers of channel uses \(n\) and the binary message of size \(k\) to be 8 and 4, respectively. These numbers are chosen this way so that we could evaluate the performance of our trained autoencoder model with the results given on \cite{o2017introduction}. Nevertheless, our covert model works independent of these parameters and can be used for any autoencoder communication setup. For training the autoencoder mode, we generate two datasets of train and test by generating random binary messages \(s\) of size \(k\). The training set contains 8192 random binary messages while this number is 51200 for the test set. We intentionally created a much larger data set for testing to make sure that each symbol \(y\) undergoes various channel distortions while evaluating the model's performance. We set the learning rate to 0.001 and optimized the model using the Adam optimizer \cite{kingma2014adam}. We chose the batch size to be 16 and trained the model for 100 epochs. For the channel configuration, we chose fixed signal to noise ratio (SNR) values during the training. The SNR value for the AWGN channel is set to 4dB, and we give a higher SNR value of 16dB to the Rayleigh fading channel due to the channel complexity. Figure () shows the performance of the trained normal communication model in terms of block error rate (BLER) for a range of SNR values under AWGN and Rayleigh fading channel conditions.\\
For the covert models, we evaluated the system on two different channel models of AWGN and Rayleigh fading. In both settings, we used the same training procedure and network architecture for our covert models. We started our experiment by sending 1 bit of covert data and gradually increased the data rate to see how increasing the covert data rate would effect each component of our covert scheme. With knowing the fact that the covert sender has to embody his covert noise signal \(\hat{z}\) to a normal signal \(x\) sent by the autoencoder communication model, we generated train and test covert messages \(m\) to have the same number of train and test messages as of the autoencoder's. The size of covert data sets are also persistent regardless of what the covert data rate is. All models are jointly trained for 5000 epochs using the Adam optimizer with the learning rate of 0.0001. In each epoch, we first update parameters of Willie's network using (\ref{willie_loss}), then train Alice's network for one step using (\ref{alice_loss}), and eventually optimize Bob's network based on (\ref{bob_loss}). Although we trained the autoencoder network on a fixed SNR value, we found when our covert scheme is trained on a range of SNR values, not only Alice learns to preserve the normal communication's accuracy better but also Bob will be able to decode the covert messages more accurately. Accordingly, we set the SNR value to be in the range of -2dB to 2dB for the AWGN channel and 5dB to 10dB for the Rayleigh fading channel.
In figure (?), you can see the results of our training progress. Figure (?) shows the progress of each covert actor's accuracy for the test set during training process. As the training goes on, Bob gradually learns to decode the covert messages \(m\) and establishes a reliable communication with Alice. After a few epochs, when the covert communication begins to take up, signals start to deviate from the distribution they had, causing Willie to better detect covert signals. When Willie's accuracy increases, the term \(\mathcal{L}_{Willie}\) dominates the other two objectives of Alice's loss function in (\ref{alice_loss}). This causes Alice to gradually sacrifice its accuracy for the sake of undetectability. Soon afterwards, the training process reaches a stable point where neither of covert models sees any noticeable improvement in their performance as the training continues. Figure (?) depicts the accuracy of Willie in percentage over a range of SNR values for flagging signals as covert and non-covert. This plot tells us how probable it is for our covert signal to be detected by a target detector at a specific SNR value. Figure (?) shows how each entity's accuracy of our covert model changes as we increase the covert rate.\\
Figure (?) compares the constellation cloud of a covert and non-covert signal for both AWGN and Rayeligh fading channels. We have marked each symbol of the encoder's signal ouput \(x\) as blue point on the constellation diagrams. Since the data is sent over 8 channel uses, there are 8 blue points on the chart.